# @package _global_
network:
  # trunk transformer config
  _target_: ppt_learning.models.policy.Policy
  embed_dim: 128
  num_blocks: 4 # num of blocks in the trunk transformer 
  num_heads: 8 # num of heads in the trunk transformer
  drop_path: 0.0 # drop path in the trunk transformer
  use_modality_embedding: True
  token_postprocessing: ${head.token_postprocessing}
  cross_stem_attention: False # use cross attention to combine state and action
  weight_init_style: 'pytorch' # weight init
  observation_horizon: ${dataset.observation_horizon}
  action_horizon: ${dataset.action_horizon}
  openloop_steps: 4
  action_dim : -1 # overwrite based on dataset
  temporal_agg: False # ACT-style temporal aggregation, not always work

head:
  _target_: ppt_learning.models.policy_head.Diffusion
  input_dim: ${network.embed_dim} # concat = 11264
  output_dim: -1 # overwrite based on dataset
  horizon: ${dataset.action_horizon}
  normalize_action: True
  down_dims: [512, 1024, 2048]
  noise_scheduler_type: "DDIM"
  num_inference_steps: 10
  token_postprocessing: "mean" # "concat" # maxpool or meanpool the tokens
  hist_horizon: 0 # whether predicting the hist action

stem:
  modalities: ["state", "image"] # no 'language'
  modality_embed_dim: ${network.embed_dim}
  num_heads: 8 # num of heads in the trunk transformer
  dim_head: 32 # dim of head in the trunk transformer
  normalize_state: True # normalize state vectors 
  cross_attention: True # whether to use cross attention or not

  # used as perceiver io to unify token sizes for each modality
  crossattn_latent:
    state: 1
    image: 2

  image:
    _target_: ppt_learning.models.policy_stem.ResNet
    resnet_model: "resnet18"
    output_dim: ${network.embed_dim}
    weights: "DEFAULT" # no weights para means no pre-trained weights are used.
    finetune: True

  # implement tokenization for state
  state:
    _target_: ppt_learning.models.policy_stem.MLP
    input_dim: 0 # ovewrite based on the dataset
    output_dim: ${network.embed_dim}
    widths: [128, 256]
